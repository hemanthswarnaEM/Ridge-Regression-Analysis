{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from numpy.linalg import det, inv\n",
    "from math import sqrt, pi\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def ldaLearn(X, y):\n",
    "    # Inputs:\n",
    "    # X - N x d matrix with each row as a training example\n",
    "    # y - N x 1 vector of labels\n",
    "    # Outputs:\n",
    "    # means - d x k matrix of class means\n",
    "    # covmat - d x d shared covariance matrix\n",
    "\n",
    "    # Get unique class labels\n",
    "    class_labels = np.unique(y)\n",
    "    k = len(class_labels)\n",
    "    d = X.shape[1]\n",
    "\n",
    "    # Initialize means matrix\n",
    "    means = np.zeros((d, k))\n",
    "\n",
    "    # Compute means for each class\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        class_data = X[y.flatten() == label, :]\n",
    "        means[:, idx] = np.mean(class_data, axis=0)\n",
    "\n",
    "    # Compute shared covariance matrix\n",
    "    covmat = np.cov(X, rowvar=False)\n",
    "\n",
    "    return means, covmat\n",
    "\n",
    "def qdaLearn(X, y):\n",
    "    # Inputs:\n",
    "    # X - N x d matrix with each row as a training example\n",
    "    # y - N x 1 vector of labels\n",
    "    # Outputs:\n",
    "    # means - d x k matrix of class means\n",
    "    # covmats - list of k d x d covariance matrices\n",
    "\n",
    "    class_labels = np.unique(y)\n",
    "    k = len(class_labels)\n",
    "    d = X.shape[1]\n",
    "\n",
    "    # Initialize means matrix and covariance list\n",
    "    means = np.zeros((d, k))\n",
    "    covmats = []\n",
    "\n",
    "    # Compute means and covariance matrices for each class\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        class_data = X[y.flatten() == label, :]\n",
    "        means[:, idx] = np.mean(class_data, axis=0)\n",
    "        covmats.append(np.cov(class_data, rowvar=False))\n",
    "\n",
    "    return means, covmats\n",
    "\n",
    "def ldaTest(means, covmat, Xtest, ytest):\n",
    "    # Inputs:\n",
    "    # means - d x k matrix of class means\n",
    "    # covmat - d x d shared covariance matrix\n",
    "    # Xtest - N x d matrix of test data\n",
    "    # ytest - N x 1 vector of true labels\n",
    "    # Outputs:\n",
    "    # ypred - N x 1 vector of predicted labels\n",
    "    # acc - accuracy percentage\n",
    "\n",
    "    inv_covmat = np.linalg.inv(covmat)\n",
    "    num_classes = means.shape[1]\n",
    "    N = Xtest.shape[0]\n",
    "\n",
    "    # Compute discriminant scores\n",
    "    scores = np.zeros((N, num_classes))\n",
    "    for idx in range(num_classes):\n",
    "        diff = Xtest - means[:, idx].T\n",
    "        scores[:, idx] = -0.5 * np.sum(diff @ inv_covmat * diff, axis=1)\n",
    "\n",
    "    # Predict labels\n",
    "    ypred = np.argmax(scores, axis=1) + 1  # +1 to match label indexing\n",
    "    ytest = ytest.flatten()\n",
    "    acc = 100 * np.mean(ypred == ytest)\n",
    "\n",
    "    return ypred.reshape(-1,1), acc\n",
    "\n",
    "def qdaTest(means, covmats, Xtest, ytest):\n",
    "    # Inputs:\n",
    "    # means - d x k matrix of class means\n",
    "    # covmats - list of k d x d covariance matrices\n",
    "    # Xtest - N x d matrix of test data\n",
    "    # ytest - N x 1 vector of true labels\n",
    "    # Outputs:\n",
    "    # ypred - N x 1 vector of predicted labels\n",
    "    # acc - accuracy percentage\n",
    "\n",
    "    num_classes = means.shape[1]\n",
    "    N = Xtest.shape[0]\n",
    "\n",
    "    # Compute discriminant scores\n",
    "    scores = np.zeros((N, num_classes))\n",
    "    for idx in range(num_classes):\n",
    "        diff = Xtest - means[:, idx].T\n",
    "        inv_covmat = np.linalg.inv(covmats[idx])\n",
    "        det_covmat = np.linalg.det(covmats[idx])\n",
    "        term1 = -0.5 * np.sum(diff @ inv_covmat * diff, axis=1)\n",
    "        term2 = -0.5 * np.log(det_covmat)\n",
    "        scores[:, idx] = term1 + term2\n",
    "\n",
    "    # Predict labels\n",
    "    ypred = np.argmax(scores, axis=1) + 1  # +1 to match label indexing\n",
    "    ytest = ytest.flatten()\n",
    "    acc = 100 * np.mean(ypred == ytest)\n",
    "\n",
    "    return ypred.reshape(-1,1), acc\n",
    "\n",
    "def learnOLERegression(X, y):\n",
    "    # Inputs:\n",
    "    # X - N x d matrix of training data\n",
    "    # y - N x 1 vector of targets\n",
    "    # Outputs:\n",
    "    # w - d x 1 vector of learned weights\n",
    "\n",
    "    w = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    return w\n",
    "\n",
    "def learnRidgeRegression(X, y, lambd):\n",
    "    # Inputs:\n",
    "    # X - N x d matrix of training data\n",
    "    # y - N x 1 vector of targets\n",
    "    # lambd - regularization parameter\n",
    "    # Outputs:\n",
    "    # w - d x 1 vector of learned weights\n",
    "\n",
    "    d = X.shape[1]\n",
    "    I = np.identity(d)\n",
    "    w = np.linalg.solve(X.T @ X + lambd * I, X.T @ y)\n",
    "    return w\n",
    "\n",
    "def testOLERegression(w, Xtest, ytest):\n",
    "    # Inputs:\n",
    "    # w - d x 1 vector of weights\n",
    "    # Xtest - N x d matrix of test data\n",
    "    # ytest - N x 1 vector of true targets\n",
    "    # Outputs:\n",
    "    # mse - mean squared error\n",
    "\n",
    "    predictions = Xtest @ w\n",
    "    mse = np.mean((ytest - predictions) ** 2)\n",
    "    return mse\n",
    "\n",
    "def regressionObjVal(w, X, y, lambd):\n",
    "    # Compute objective function value and gradient for ridge regression\n",
    "    # Inputs:\n",
    "    # w - d x 1 vector of weights\n",
    "    # X - N x d matrix of data\n",
    "    # y - N x 1 vector of targets\n",
    "    # lambd - regularization parameter\n",
    "    # Outputs:\n",
    "    # error - scalar value of the objective function\n",
    "    # error_grad - d x 1 vector of gradients\n",
    "\n",
    "    w = w.reshape(-1, 1)\n",
    "    error = 0.5 * np.sum((y - X @ w) ** 2) + 0.5 * lambd * np.sum(w ** 2)\n",
    "    error_grad = -X.T @ (y - X @ w) + lambd * w\n",
    "    error_grad = error_grad.flatten()\n",
    "    return error, error_grad\n",
    "\n",
    "def mapNonLinear(x, p):\n",
    "    # Inputs:\n",
    "    # x - N x 1 vector\n",
    "    # p - degree of polynomial\n",
    "    # Outputs:\n",
    "    # Xp - N x (p+1) matrix of mapped data\n",
    "\n",
    "    N = x.shape[0]\n",
    "    Xp = np.ones((N, p+1))\n",
    "    for i in range(1, p+1):\n",
    "        Xp[:, i] = x.flatten() ** i  # Ensure x is 1D for exponentiation\n",
    "    return Xp\n",
    "\n",
    "# Main script\n",
    "\n",
    "# Problem 1\n",
    "# Load the sample data\n",
    "if sys.version_info.major == 2:\n",
    "    X, y, Xtest, ytest = pickle.load(open('basecode/sample.pickle', 'rb'))\n",
    "else:\n",
    "    X, y, Xtest, ytest = pickle.load(open('basecode/sample.pickle', 'rb'), encoding='latin1')\n",
    "\n",
    "# LDA\n",
    "means, covmat = ldaLearn(X, y)\n",
    "ldares, ldaacc = ldaTest(means, covmat, Xtest, ytest)\n",
    "print('LDA Accuracy = ' + str(ldaacc))\n",
    "\n",
    "# QDA\n",
    "means, covmats = qdaLearn(X, y)\n",
    "qdares, qdaacc = qdaTest(means, covmats, Xtest, ytest)\n",
    "print('QDA Accuracy = ' + str(qdaacc))\n",
    "\n",
    "# Plotting boundaries\n",
    "cmap_background = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#FFD700'])\n",
    "\n",
    "# Define grid for decision boundaries\n",
    "x1 = np.linspace(-5, 20, 100)\n",
    "x2 = np.linspace(-5, 20, 100)\n",
    "xx1, xx2 = np.meshgrid(x1, x2)\n",
    "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "fig = plt.figure(figsize=[12, 6])\n",
    "\n",
    "# Plot LDA decision boundaries\n",
    "plt.subplot(1, 2, 1)\n",
    "zldares, _ = ldaTest(means, covmat, grid_points, np.zeros((grid_points.shape[0], 1)))\n",
    "plt.contourf(x1, x2, zldares.reshape(xx1.shape), alpha=0.3, cmap=cmap_background)\n",
    "plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest.flatten(), edgecolor='k', cmap=cmap_background)\n",
    "plt.title('LDA Decision Boundary')\n",
    "\n",
    "# Plot QDA decision boundaries\n",
    "plt.subplot(1, 2, 2)\n",
    "zqdares, _ = qdaTest(means, covmats, grid_points, np.zeros((grid_points.shape[0], 1)))\n",
    "plt.contourf(x1, x2, zqdares.reshape(xx1.shape), alpha=0.3, cmap=cmap_background)\n",
    "plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest.flatten(), edgecolor='k', cmap=cmap_background)\n",
    "plt.title('QDA Decision Boundary')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Problem 2\n",
    "if sys.version_info.major == 2:\n",
    "    X, y, Xtest, ytest = pickle.load(open('basecode/diabetes.pickle', 'rb'))\n",
    "else:\n",
    "    X, y, Xtest, ytest = pickle.load(open('basecode/diabetes.pickle', 'rb'), encoding='latin1')\n",
    "\n",
    "# Add intercept\n",
    "X_i = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "Xtest_i = np.hstack((np.ones((Xtest.shape[0], 1)), Xtest))\n",
    "\n",
    "# Learn weights without intercept\n",
    "w = learnOLERegression(X, y)\n",
    "# Calculate MSE without intercept for training and testing\n",
    "mse_train_without_intercept = testOLERegression(w, X, y)\n",
    "mse_test_without_intercept = testOLERegression(w, Xtest, ytest)\n",
    "\n",
    "# Learn weights with intercept\n",
    "w_i = learnOLERegression(X_i, y)\n",
    "# Calculate MSE with intercept for training and testing\n",
    "mse_train_with_intercept = testOLERegression(w_i, X_i, y)\n",
    "mse_test_with_intercept = testOLERegression(w_i, Xtest_i, ytest)\n",
    "\n",
    "# Print results\n",
    "print('\\nMSE without intercept for training:', mse_train_without_intercept)\n",
    "print('MSE with intercept for training:', mse_train_with_intercept)\n",
    "print('MSE without intercept for testing:', mse_test_without_intercept)\n",
    "print('MSE with intercept for testing:', mse_test_with_intercept)\n",
    "\n",
    "# Problem 3\n",
    "k = 101\n",
    "lambdas = np.linspace(0, 1, num=k)\n",
    "mses3_train = np.zeros((k, 1))\n",
    "mses3 = np.zeros((k, 1))\n",
    "\n",
    "print('\\nMSE VALUES OF TRAIN AND TEST DATA USING LAMBDA VALUES 0-1')\n",
    "for i, lambd in enumerate(lambdas):\n",
    "    w_l = learnRidgeRegression(X_i, y, lambd)\n",
    "    mses3_train[i] = testOLERegression(w_l, X_i, y)\n",
    "    mses3[i] = testOLERegression(w_l, Xtest_i, ytest)\n",
    "    print(f\"Lambda: {lambd:.2f}, Train MSE: {mses3_train[i][0]:.4f}, Test MSE: {mses3[i][0]:.4f}\")\n",
    "\n",
    "fig = plt.figure(figsize=[12, 6])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lambdas, mses3_train)\n",
    "plt.title('MSE for Train Data')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lambdas, mses3)\n",
    "plt.title('MSE for Test Data')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(w_i, marker='o')\n",
    "plt.title('Weights for OLE Regression with Intercept')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.show()\n",
    "\n",
    "# Select the optimal lambda value from Problem 3\n",
    "lambda_opt = lambdas[np.argmin(mses3)]\n",
    "print(f\"\\nOptimal lambda based on test MSE in Problem 3: {lambda_opt}\")\n",
    "\n",
    "# Compute Ridge Regression weights using the selected lambda\n",
    "w_ridge = learnRidgeRegression(X_i, y, lambda_opt)\n",
    "\n",
    "# Plot Ridge Regression Weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(w_ridge, marker='o')\n",
    "plt.title('Weights for Ridge Regression with Intercept')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.show()\n",
    "\n",
    "# Problem 4\n",
    "mses4_train = np.zeros((k, 1))\n",
    "mses4 = np.zeros((k, 1))\n",
    "opts = {'maxiter': 100}\n",
    "w_init = np.zeros(X_i.shape[1])  # Corrected to 1D array\n",
    "\n",
    "print(\"\\nMSE VALUES OF TRAIN AND TEST DATA USING GRADIENT DESCENT APPROACH\")\n",
    "for i, lambd in enumerate(lambdas):\n",
    "    args = (X_i, y, lambd)\n",
    "    res = minimize(regressionObjVal, w_init, jac=True, args=args, method='CG', options=opts)\n",
    "    w_l = res.x.reshape(-1, 1)\n",
    "    mses4_train[i] = testOLERegression(w_l, X_i, y)\n",
    "    mses4[i] = testOLERegression(w_l, Xtest_i, ytest)\n",
    "    print(f\"Lambda: {lambd:.2f}, Train MSE: {mses4_train[i][0]:.4f}, Test MSE: {mses4[i][0]:.4f}\")\n",
    "\n",
    "fig = plt.figure(figsize=[12, 6])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lambdas, mses4_train, label='Using scipy.minimize')\n",
    "plt.plot(lambdas, mses3_train, label='Direct minimization')\n",
    "plt.title('MSE for Train Data')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lambdas, mses4, label='Using scipy.minimize')\n",
    "plt.plot(lambdas, mses3, label='Direct minimization')\n",
    "plt.title('MSE for Test Data')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Problem 5\n",
    "pmax = 7\n",
    "mses5_train = np.zeros((pmax, 2))\n",
    "mses5 = np.zeros((pmax, 2))\n",
    "\n",
    "print(\"\\nMSE VALUES FOR TRAIN AND TEST DATA USING DIFFERENT P VALUES (WITH AND WITHOUT REGULARIZATION)\")\n",
    "for p in range(pmax):\n",
    "    Xd = mapNonLinear(X[:, 2:3], p)\n",
    "    Xdtest = mapNonLinear(Xtest[:, 2:3], p)\n",
    "\n",
    "    # Without regularization\n",
    "    w_d1 = learnRidgeRegression(Xd, y, 0)\n",
    "    mses5_train[p, 0] = testOLERegression(w_d1, Xd, y)\n",
    "    mses5[p, 0] = testOLERegression(w_d1, Xdtest, ytest)\n",
    "\n",
    "    # With regularization\n",
    "    w_d2 = learnRidgeRegression(Xd, y, lambda_opt)\n",
    "    mses5_train[p, 1] = testOLERegression(w_d2, Xd, y)\n",
    "    mses5[p, 1] = testOLERegression(w_d2, Xdtest, ytest)\n",
    "\n",
    "    print(f\"P value: {p}\")\n",
    "    print(f\"Train MSE without regularization: {mses5_train[p, 0]:.4f}\")\n",
    "    print(f\"Train MSE with regularization (lambda={lambda_opt}): {mses5_train[p, 1]:.4f}\")\n",
    "    print(f\"Test MSE without regularization: {mses5[p, 0]:.4f}\")\n",
    "    print(f\"Test MSE with regularization (lambda={lambda_opt}): {mses5[p, 1]:.4f}\\n\")\n",
    "\n",
    "# Determine the optimal p values based on the lowest test MSE\n",
    "optimal_p_without_reg = np.argmin(mses5[:, 0])\n",
    "optimal_p_with_reg = np.argmin(mses5[:, 1])\n",
    "\n",
    "# Print the optimal p values\n",
    "print(\"Optimal p values based on the lowest Test MSE:\")\n",
    "print(f\"Optimal p without regularization: {optimal_p_without_reg}\")\n",
    "print(f\"Optimal p with regularization (lambda={lambda_opt}): {optimal_p_with_reg}\")\n",
    "\n",
    "# Plotting MSE for Train and Test Data\n",
    "fig = plt.figure(figsize=[12, 6])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(pmax), mses5_train[:, 0], label='No Regularization')\n",
    "plt.plot(range(pmax), mses5_train[:, 1], label='Regularization')\n",
    "plt.title('MSE for Train Data')\n",
    "plt.xlabel('Polynomial Degree p')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(pmax), mses5[:, 0], label='No Regularization')\n",
    "plt.plot(range(pmax), mses5[:, 1], label='Regularization')\n",
    "plt.title('MSE for Test Data')\n",
    "plt.xlabel('Polynomial Degree p')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Additional Insights and Graphs\n",
    "\n",
    "# 1. Comparison of OLE and Ridge Regression Weights\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(w_i, marker='o', label='OLE Weights')\n",
    "plt.plot(w_ridge, marker='x', label='Ridge Weights')\n",
    "plt.title('Comparison of Weights: OLE vs Ridge Regression')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Residual Plot for Ridge Regression\n",
    "predictions = Xtest_i @ w_ridge\n",
    "residuals = ytest - predictions\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(predictions, residuals, alpha=0.5)\n",
    "plt.hlines(y=0, xmin=predictions.min(), xmax=predictions.max(), colors='r')\n",
    "plt.title('Residual Plot for Ridge Regression')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 3. Learning Curve for Ridge Regression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    Ridge(alpha=lambda_opt), X_i, y.flatten(), cv=5, scoring='neg_mean_squared_error',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "test_errors = -test_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_errors, label='Training Error')\n",
    "plt.plot(train_sizes, test_errors, label='Cross-Validation Error')\n",
    "plt.title('Learning Curve for Ridge Regression')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
